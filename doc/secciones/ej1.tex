\section{Ejercicio 1}

En este ejercicio se pide la selección de un subconjunto de números de manera tal que su suma sea la más cercana a un número P dado.
Dicha suma debe ser menor o igual a P.

Podemos notar que es una variación del problema clásico conocido como \textbf{Subset Sum} y por lo tanto probaremos que se puede resolver con la misma complejidad.

El algoritmo consiste en dividir la entrada en dos subgrupos de tamaño $\lceil n/2 \rceil$ y $\lfloor n/2 \rfloor$.
Para el primer subgrupo calcular todas las posibles sumas para todos los subconjuntos posibles ($2^{n/2}$) y guardas las que sean menores a P en un conjunto que permita la consulta de la existencia de un elemento o bien que nos devuelva el primero más chico al buscado.

Para el segundo subgrupo, nuevamente calcular todas las posibles sumas para todos los subconjuntos posibles y en caso de ser menor estricto a P, buscar en la estructura anterior el primer número que sea menor o igual a (P - la suma acumulada). Ese número que nos devuelva sumado a la suma que nos dio el subconjunto calculado son una posible respuesta. 

Con el almacenamiento de una variable global que posea el máximo número alcanzado hasta el momento, ante cada consulta anterior me quedo siempre con el máximo hasta el momento y el nuevo valor encontrado. 

Basta con reportar esta variable global que es la respuesta al problema que estábamos buscando.

\subsection{Cómo cumplir con las complejidades pedidas}

Para lograr la complejidad de \bigo($n * 2 ^{n/2}$) basta con que la primer y la segunda parte del algoritmo tarden dicha complejidad.
La primera parte itera sobre $2 ^{n/2}$ subconjuntos y en cada uno de ellos calcula n/2 sumas. A su vez inserta en un conjunto (que lo implementaremos con un set de C++) el resultado de la suma. Esta última operación tarda \bigo(log M) donde M es la cantidad de elementos del conjunto. Como en nuestro conjunto tendremos almacenados hasta $2 ^{n/2}$ elementos su logaritmo es n/2 y entonces por cada iteración del ciclo se realizan en promedio n operaciones.
Con esto concluimos que la primer parte del algoritmo tiene complejidad  \bigo($n * 2 ^{n/2}$).

Para la segunda parte del algoritmo nuevamente tenemos $2 ^{n/2}$ iteraciones para las cuales haremos la misma cantidad de sumas que antes (n/2) y se deberá hacer una búsqueda sobre el conjunto. En caso de que la suma parcial ya sea mayor al target, ya no sirve continuar porque nos pasamos. Caso contrario, usamos la operación que nos provee el conjunto implementado sobre set llamada upper_bound que en tiempo logarítmico devuelve el primer elemento $>$ al target buscado. Como buscamos el primer elemento que sea menor o igual, retrocedemos en uno el iterador y llegamos al elemento que estábamos buscando (vale aclarar que como siempre está el número 0 en el conjunto, podemos asegurar que siempre habrá un elemento menor o igual). Estas dos operaciones tardan tiempo logarítmico sobre la cantidad de elementos del conjunto, y como bien habíamos dicho en el párrafo anterior la cantidad total era de $2 ^{n/2}$ elementos y entonces su logaritmo es n/2. Nuevamente tardamos \bigo($n * 2 ^{n/2}$).

El algoritmo termina con la ejecución de esos dos ciclos y la devolución de la respuesta en \bigo(1) por lo que la complejidad total se resume en \bigo($n * 2 ^{n/2}$).

Para testear el algoritmo se probaron todas instancias aleatorias para cada uno de los n posibles.
